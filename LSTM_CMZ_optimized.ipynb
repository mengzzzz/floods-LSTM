{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for Time Series Prediction\n",
    "\n",
    "This notebook demonstrates how to use a Long Short-Term Memory (LSTM) network for time series forecasting. \n",
    "It includes data preprocessing, model training, evaluation, and visualization.\n",
    "\n",
    "## Steps Covered:\n",
    "1. Load and preprocess the dataset\n",
    "2. Create the LSTM model\n",
    "3. Train the model\n",
    "4. Evaluate performance\n",
    "5. Visualize results\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7.15 (default, Nov 24 2022, 18:44:54) [MSC v.1916 64 bit (AMD64)]\n",
      "Numpy version: 1.21.6\n",
      "Scipy version: 1.7.3\n",
      "Pandas version: 1.3.5\n",
      "Pastas version: 0.21.0\n",
      "Matplotlib version: 3.5.3\n"
     ]
    }
   ],
   "source": [
    "import warnings  # Import necessary libraries\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "## Import libraries developed in this study\n",
    "# import hydrodeepx_data as xdata  # Import necessary libraries\n",
    "# import hydrodeepx_utils as xutils  # Import necessary libraries\n",
    "# import hydrodeepx_interpret as xinterpret  # Import necessary libraries\n",
    "# import hydrodeepx_plot as xplot  # Import necessary libraries\n",
    "from hydrodata import DataforIndividual  # Import necessary libraries\n",
    "## Import dependent libraries\n",
    "import os, logging, pickle  # Import necessary libraries\n",
    "import numpy as np  # Import necessary libraries\n",
    "import pandas as pd  # Import necessary libraries\n",
    "import matplotlib.pyplot as plt  # Import necessary libraries\n",
    "import tensorflow as tf  # Import necessary libraries\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers, regularizers  # Import necessary libraries\n",
    "from tensorflow.keras import backend as K  # Import necessary libraries\n",
    "## Ignore all the warnings\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['KMP_WARNINGS'] = '0'\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "import pastas as ps  # Import necessary libraries\n",
    "import matplotlib.pyplot as plt  # Import necessary libraries\n",
    "import spi  # Import necessary libraries\n",
    "\n",
    "ps.set_log_level(\"ERROR\")\n",
    "ps.show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating index\n",
    "\n",
    "def cal_nse(obs, sim):\n",
    "\n",
    "    # compute numerator and denominator\n",
    "    numerator   = np.nansum((obs - sim)**2)\n",
    "    denominator = np.nansum((obs - np.nanmean(obs))**2)\n",
    "    # compute coefficient\n",
    "    return 1 - (numerator / denominator)\n",
    "\n",
    "def R2(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "\n",
    "def calculate_rmse(observed, predicted):  # Make predictions\n",
    "    mse = np.mean((observed - predicted)**2)  # Make predictions\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing functions.\n",
    "#read, wrap and normalize the data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_station_data(fname):\n",
    "    \"\"\"\n",
    "    Obtain the pandas dataframe from dataset.\n",
    "\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(fname)  # Load dataset\n",
    "    dataset[\"date\"] = pd.to_datetime(dataset['date'], dayfirst=True,errors='ignore')\n",
    "    #dataset = dataset.replace(-99, np.NaN)\n",
    "    dataset = dataset.set_index(\"date\")\n",
    "    dataset = dataset.dropna()\n",
    "    \n",
    "    # This date range is a fake date that helps better arrange the data.\n",
    "    # This can be changed with orders\n",
    "    dataset = dataset.loc[pd.date_range(start='1/1/1968', end='1/1/2018')]\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_wrapped_data(dataset, wrap_length=12):\n",
    "    \"\"\"\n",
    "    Wrap the data for the shape requirement of LSTM.  # Define LSTM model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: the pandas dataframe obtained from the function get_station_data().\n",
    "    wrap_length: the number of time steps to be considered for the LSTM layer.  # Define LSTM model\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    data_x_dict: the input dictionary whose key is the date and value is the corresponding wrapped input matrix of each sample.\n",
    "    data_y_dict: the output dictionary whose key is the date and value is the corresponding target of each sample.\n",
    "    \"\"\"\n",
    "    data_x_dict, data_y_dict = {}, {}\n",
    "\n",
    "    for date_i in tqdm(dataset.index, desc=f'Prep aring data with wrap length = {wrap_length}'):\n",
    "        try:\n",
    "            data_x = dataset.loc[pd.date_range(end=date_i,\n",
    "                                               periods=wrap_length + 1,\n",
    "                                               freq=\"d\")[:-1], [\"R\"], ].to_numpy(dtype='float16')\n",
    "            data_y = dataset.loc[pd.date_range(end=date_i,\n",
    "                                               periods=wrap_length + 1,\n",
    "                                               freq=\"d\")[-1:], \"y\", ].to_numpy(dtype='float16')\n",
    "\n",
    "            data_x_dict[date_i] = data_x\n",
    "            data_y_dict[date_i] = data_y\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    return data_x_dict, data_y_dict\n",
    "\n",
    "\n",
    "def split_train_test(dataset, data_x_dict, data_y_dict, frac=0.8, random_state=100, scale=True):\n",
    "    \"\"\"\n",
    "    Randomly split the dataset for training and testing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: the pandas dataframe obtained from the function get_station_data().\n",
    "    data_x_dict: the input dictionary obtained from the function get_wrapped_data().\n",
    "    data_y_dict: the output dictionary obtained from the function get_wrapped_data().\n",
    "    frac: the fraction of samples to be trained.\n",
    "    random_state: the random seed (default: 100).\n",
    "    scale: [bool] whether scale the split dataset by the mean and std values of the training data (default: True).\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    train_dates: the dates of the picked training data.\n",
    "    test_dates: the dates of the picked testing data.\n",
    "    train_x: the (scaled) inputs for training.\n",
    "    train_y: the (scaled) outputs for training.\n",
    "    test_x: the (scaled) inputs for testing.\n",
    "    test_y: the (scaled) outputs for testing.\n",
    "    scale_params: the mean and std values of the training data (available when scale is True)\n",
    "    \"\"\"\n",
    "    train_dates = (dataset.loc[data_x_dict.keys()].sample(frac=frac, random_state=random_state).index)\n",
    "    test_dates  = dataset.loc[data_x_dict.keys()].drop(train_dates).index\n",
    "\n",
    "    train_x = np.stack([data_x_dict.get(i) for i in train_dates.to_list()])\n",
    "    train_y = np.stack([data_y_dict.get(i) for i in train_dates.to_list()])\n",
    "    test_x  = np.stack([data_x_dict.get(i) for i in test_dates.to_list()])\n",
    "    test_y  = np.stack([data_y_dict.get(i) for i in test_dates.to_list()])\n",
    "\n",
    "    scale_params = {\"train_x_mean\": 0, \"train_x_std\": 1, \"train_y_mean\": 0, \"train_y_std\": 1}\n",
    "\n",
    "    if scale is False:\n",
    "        return train_dates, test_dates, train_x, train_y, test_x, test_y, scale_params\n",
    "    else:\n",
    "        scale_params[\"train_x_mean\"] = (dataset.loc[train_dates, [\"R\"]].mean().values)\n",
    "        scale_params[\"train_x_std\"]  = (dataset.loc[train_dates, [\"R\"]].std().values)\n",
    "        scale_params[\"train_y_mean\"] = dataset.loc[train_dates, [\"y\"]].mean().values\n",
    "        scale_params[\"train_y_std\"]  = dataset.loc[train_dates, [\"y\"]].std().values\n",
    "\n",
    "        train_x = (train_x - scale_params[\"train_x_mean\"][None, None, :]) / scale_params[\"train_x_std\"][None, None, :]\n",
    "        train_y = (train_y - scale_params[\"train_y_mean\"][None, :]) / scale_params[\"train_y_std\"][None, :]\n",
    "        test_x  = (test_x - scale_params[\"train_x_mean\"][None, None, :]) / scale_params[\"train_x_std\"][None, None, :]\n",
    "        test_y  = (test_y - scale_params[\"train_y_mean\"][None, :]) / scale_params[\"train_y_std\"][None, :]\n",
    "\n",
    "        return train_dates, test_dates, train_x, train_y, test_x, test_y, scale_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(r\"C:\\TsaiHejiang\\Codes\\GRU_GW\\03\\NEW\\Results\\Timeseries\\02296500.csv\",  parse_dates=['date'],  # Load dataset\n",
    "#                    index_col='date', squeeze=True)\n",
    "# data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GW = data['GW(mm)']\n",
    "# prcp = data['prcp(mm/day)']\n",
    "# Tmean = data['tmean(C)']\n",
    "# Stream = data['flow(mm)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_PATH  = r\"C:\\TsaiHejiang\\CMZ\"\n",
    "\n",
    "####################\n",
    "#   Basin set up   #\n",
    "####################\n",
    "STATION_ID = '40-year-daily_#8002_1' # USGS code used in the MOPEX dataset\n",
    "\n",
    "####################\n",
    "#  Hyperparameters #\n",
    "####################\n",
    "RANDOM_SEED   = 100        \n",
    "WRAP_LENGTH   = 180        # Timestep of the LSTM model  # Define LSTM model\n",
    "TRAIN_FRAC    = 0.7        # The fraction of spliting traning and testing dataset\n",
    "\n",
    "LEARNING_RATE = 0.03\n",
    "EPOCH_NUMBER  = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mopex_path = os.path.join(WORKING_PATH, f'{STATION_ID}.csv')\n",
    "data_path  = os.path.join(WORKING_PATH, 'data', f'{STATION_ID}_data.pickle')\n",
    "model_path = os.path.join(WORKING_PATH, 'results', 'model', f'{STATION_ID}_{RANDOM_SEED}_keras.h5')\n",
    "eg_path    = os.path.join(WORKING_PATH, 'results', 'eg',    f'{STATION_ID}_{RANDOM_SEED}_eg.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                y    R\n",
      "date                  \n",
      "1968-01-01  14.83  7.6\n",
      "1968-01-02  13.98  3.7\n",
      "1968-01-03  12.79  4.7\n",
      "1968-01-04  12.45  1.8\n",
      "1968-01-05  11.80  0.3\n",
      "...           ...  ...\n",
      "2017-12-28  15.67  0.4\n",
      "2017-12-29  13.96  2.5\n",
      "2017-12-30  13.41  8.4\n",
      "2017-12-31  19.77  3.8\n",
      "2018-01-01  22.03  3.8\n",
      "\n",
      "[18264 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# hydrodata = xdata.get_station_data(fname=mopex_path)\n",
    "hydrodata = get_station_data(fname=mopex_path)\n",
    "print(hydrodata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, [ax1, ax2, ax3] = plt.subplots(nrows=3, ncols=1, sharex=True, figsize=(8, 5))  # Visualize results\n",
    "# fig.tight_layout()\n",
    "\n",
    "# ax1.plot(hydrodata['prcp'],  'tab:blue', lw=0.8)  # Visualize results\n",
    "# ax2.plot(hydrodata['tmean'], 'tab:red',  lw=0.8)  # Visualize results\n",
    "# ax3.plot(hydrodata['flow'],  'tab:brown', lw=0.8)  # Visualize results\n",
    "\n",
    "# ax1.set_title(f\"Station {STATION_ID}\")\n",
    "# ax1.set_ylabel(\"prcp(mm/day)\")\n",
    "# ax2.set_ylabel(\"tmean(C)\")\n",
    "# ax3.set_ylabel(\"flow(mm)\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['1968-01-01', '1968-01-02', '1968-01-03', '1968-01-04',\n",
      "               '1968-01-05', '1968-01-06', '1968-01-07', '1968-01-08',\n",
      "               '1968-01-09', '1968-01-10',\n",
      "               ...\n",
      "               '2017-12-23', '2017-12-24', '2017-12-25', '2017-12-26',\n",
      "               '2017-12-27', '2017-12-28', '2017-12-29', '2017-12-30',\n",
      "               '2017-12-31', '2018-01-01'],\n",
      "              dtype='datetime64[ns]', name='date', length=18264, freq=None)\n"
     ]
    }
   ],
   "source": [
    "print(hydrodata.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm  # Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa19c81f64c4e9499c2526832dc507c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prep aring data with wrap length = 180:   0%|          | 0/18264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if os.path.exists(data_path):\n",
    "    with open(data_path, 'rb') as f:\n",
    "        data_x_dict, data_y_dict = pickle.load(f)\n",
    "else:\n",
    "    data_x_dict, data_y_dict = get_wrapped_data(dataset=hydrodata,  wrap_length=WRAP_LENGTH)\n",
    "    with open(data_path, 'wb') as f:\n",
    "        pickle.dump([data_x_dict, data_y_dict], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of x_train, y_train after wrapping by 180 days are (12659, 180, 1), (12659, 1)\n",
      "The shape of x_test, y_test after wrapping by 180 days are   (5425, 180, 1), (5425, 1)\n"
     ]
    }
   ],
   "source": [
    "split_results = split_train_test(dataset=hydrodata, \n",
    "                                       data_x_dict=data_x_dict, \n",
    "                                       data_y_dict=data_y_dict, \n",
    "                                       frac=TRAIN_FRAC, \n",
    "                                       random_state=RANDOM_SEED, \n",
    "                                       scale=True)\n",
    "\n",
    "train_dates, test_dates, x_train, y_train, x_test, y_test, scale_params = split_results\n",
    "\n",
    "print(f'The shape of x_train, y_train after wrapping by {WRAP_LENGTH} days are {x_train.shape}, {y_train.shape}')\n",
    "print(f'The shape of x_test, y_test after wrapping by {WRAP_LENGTH} days are   {x_test.shape}, {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 180, 1)]          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 16)                1152      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 1,168\n",
      "Trainable params: 1,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(x_train.shape[1:], name='input')\n",
    "lstm   = layers.LSTM(units=16, name='lstm',   # Define LSTM model\n",
    "                     kernel_regularizer=regularizers.l2(0.001), \n",
    "                     recurrent_regularizer=regularizers.l2(0.001))(inputs)\n",
    "output = layers.Dense(units=1, name='dense', activation='linear', use_bias=False, \n",
    "                      kernel_regularizer=regularizers.l2(0.001))(lstm)\n",
    "\n",
    "model  = models.Model(inputs, output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import signal  # Import necessary libraries\n",
    "# from tensorflow.keras import backend as K  # Import necessary libraries\n",
    "# import tensorflow as tf  # Import necessary libraries\n",
    "# from tensorflow.keras import layers, models, callbacks, optimizers, regularizers  # Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8861 samples, validate on 3798 samples\n",
      "Epoch 1/100\n",
      "8861/8861 [==============================] - 3s 363us/sample - loss: 0.6065 - R2: 0.4081 - val_loss: 0.4645 - val_R2: 0.5586\n",
      "Epoch 2/100\n",
      "8861/8861 [==============================] - 2s 199us/sample - loss: 0.4473 - R2: 0.5718 - val_loss: 0.4106 - val_R2: 0.6201\n",
      "Epoch 3/100\n",
      "8861/8861 [==============================] - 2s 215us/sample - loss: 0.4255 - R2: 0.5920 - val_loss: 0.3900 - val_R2: 0.6349\n",
      "Epoch 4/100\n",
      "8861/8861 [==============================] - 2s 212us/sample - loss: 0.4021 - R2: 0.6165 - val_loss: 0.3795 - val_R2: 0.6456\n",
      "Epoch 5/100\n",
      "8861/8861 [==============================] - 2s 201us/sample - loss: 0.3955 - R2: 0.6157 - val_loss: 0.3926 - val_R2: 0.6284\n",
      "Epoch 6/100\n",
      "8861/8861 [==============================] - 2s 205us/sample - loss: 0.4068 - R2: 0.6099 - val_loss: 0.3784 - val_R2: 0.6431\n",
      "Epoch 7/100\n",
      "8861/8861 [==============================] - 2s 202us/sample - loss: 0.4010 - R2: 0.6143 - val_loss: 0.3687 - val_R2: 0.6486\n",
      "Epoch 8/100\n",
      "8861/8861 [==============================] - 2s 197us/sample - loss: 0.3918 - R2: 0.6213 - val_loss: 0.3676 - val_R2: 0.6500\n",
      "Epoch 9/100\n",
      "8861/8861 [==============================] - 2s 197us/sample - loss: 0.3827 - R2: 0.6224 - val_loss: 0.3504 - val_R2: 0.6601\n",
      "Epoch 10/100\n",
      "8861/8861 [==============================] - 2s 199us/sample - loss: 0.3845 - R2: 0.6268 - val_loss: 0.3812 - val_R2: 0.6345\n",
      "Epoch 11/100\n",
      "8861/8861 [==============================] - 2s 203us/sample - loss: 0.3932 - R2: 0.6146 - val_loss: 0.3580 - val_R2: 0.6640\n",
      "Epoch 12/100\n",
      "8861/8861 [==============================] - 2s 207us/sample - loss: 0.3788 - R2: 0.6370 - val_loss: 0.3585 - val_R2: 0.6607\n",
      "Epoch 13/100\n",
      "8861/8861 [==============================] - 2s 209us/sample - loss: 0.3763 - R2: 0.6369 - val_loss: 0.3397 - val_R2: 0.6753\n",
      "Epoch 14/100\n",
      "8861/8861 [==============================] - 2s 195us/sample - loss: 0.3675 - R2: 0.6404 - val_loss: 0.3499 - val_R2: 0.6670\n",
      "Epoch 15/100\n",
      "8861/8861 [==============================] - 2s 200us/sample - loss: 0.3729 - R2: 0.6420 - val_loss: 0.3537 - val_R2: 0.6661\n",
      "Epoch 16/100\n",
      "8861/8861 [==============================] - 2s 204us/sample - loss: 0.3705 - R2: 0.6407 - val_loss: 0.3447 - val_R2: 0.6758\n",
      "Epoch 17/100\n",
      "8861/8861 [==============================] - 2s 209us/sample - loss: 0.3676 - R2: 0.6432 - val_loss: 0.3415 - val_R2: 0.6804\n",
      "Epoch 18/100\n",
      "8861/8861 [==============================] - 2s 196us/sample - loss: 0.3655 - R2: 0.6448 - val_loss: 0.3463 - val_R2: 0.6660\n",
      "Epoch 19/100\n",
      "8861/8861 [==============================] - 2s 196us/sample - loss: 0.3621 - R2: 0.6504 - val_loss: 0.3370 - val_R2: 0.6789\n",
      "Epoch 20/100\n",
      "8861/8861 [==============================] - 2s 202us/sample - loss: 0.3642 - R2: 0.6453 - val_loss: 0.3361 - val_R2: 0.6746\n",
      "Epoch 21/100\n",
      "8861/8861 [==============================] - 2s 215us/sample - loss: 0.3632 - R2: 0.6440 - val_loss: 0.3413 - val_R2: 0.6804\n",
      "Epoch 22/100\n",
      "8861/8861 [==============================] - 2s 204us/sample - loss: 0.3631 - R2: 0.6458 - val_loss: 0.3421 - val_R2: 0.6749\n",
      "Epoch 23/100\n",
      "8861/8861 [==============================] - 2s 198us/sample - loss: 0.3712 - R2: 0.6363 - val_loss: 0.3402 - val_R2: 0.6777\n",
      "Epoch 24/100\n",
      "8861/8861 [==============================] - 2s 195us/sample - loss: 0.3766 - R2: 0.6301 - val_loss: 0.3509 - val_R2: 0.6696\n",
      "Epoch 25/100\n",
      "8861/8861 [==============================] - 2s 209us/sample - loss: 0.3741 - R2: 0.6403 - val_loss: 0.3472 - val_R2: 0.6725\n",
      "Epoch 26/100\n",
      "8861/8861 [==============================] - 2s 218us/sample - loss: 0.3674 - R2: 0.6468 - val_loss: 0.3429 - val_R2: 0.6726\n",
      "Epoch 27/100\n",
      "8861/8861 [==============================] - 2s 226us/sample - loss: 0.3633 - R2: 0.6468 - val_loss: 0.3403 - val_R2: 0.6729\n",
      "Epoch 28/100\n",
      "8192/8861 [==========================>...] - ETA: 0s - loss: 0.3620 - R2: 0.6518\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.014999999664723873.\n",
      "8861/8861 [==============================] - 2s 222us/sample - loss: 0.3660 - R2: 0.6417 - val_loss: 0.3361 - val_R2: 0.6811\n",
      "Epoch 29/100\n",
      "8861/8861 [==============================] - 2s 217us/sample - loss: 0.3582 - R2: 0.6546 - val_loss: 0.3303 - val_R2: 0.6929\n",
      "Epoch 30/100\n",
      "8861/8861 [==============================] - 2s 227us/sample - loss: 0.3540 - R2: 0.6540 - val_loss: 0.3265 - val_R2: 0.6898\n",
      "Epoch 31/100\n",
      "8861/8861 [==============================] - 2s 220us/sample - loss: 0.3562 - R2: 0.6566 - val_loss: 0.3281 - val_R2: 0.6906\n",
      "Epoch 32/100\n",
      "8861/8861 [==============================] - 2s 201us/sample - loss: 0.3519 - R2: 0.6631 - val_loss: 0.3244 - val_R2: 0.6959\n",
      "Epoch 33/100\n",
      "8861/8861 [==============================] - 2s 204us/sample - loss: 0.3568 - R2: 0.6536 - val_loss: 0.3335 - val_R2: 0.6878\n",
      "Epoch 34/100\n",
      "8861/8861 [==============================] - 2s 198us/sample - loss: 0.3624 - R2: 0.6517 - val_loss: 0.3425 - val_R2: 0.6636\n",
      "Epoch 35/100\n",
      "8861/8861 [==============================] - 2s 203us/sample - loss: 0.3641 - R2: 0.6449 - val_loss: 0.3341 - val_R2: 0.6783\n",
      "Epoch 36/100\n",
      "8861/8861 [==============================] - 2s 202us/sample - loss: 0.3595 - R2: 0.6514 - val_loss: 0.3327 - val_R2: 0.6804\n",
      "Epoch 37/100\n",
      "8861/8861 [==============================] - 2s 230us/sample - loss: 0.3554 - R2: 0.6542 - val_loss: 0.3283 - val_R2: 0.6951\n",
      "Epoch 38/100\n",
      "8861/8861 [==============================] - 2s 226us/sample - loss: 0.3538 - R2: 0.6579 - val_loss: 0.3277 - val_R2: 0.6872\n",
      "Epoch 39/100\n",
      "8861/8861 [==============================] - 2s 236us/sample - loss: 0.3548 - R2: 0.6555 - val_loss: 0.3283 - val_R2: 0.6959\n",
      "Epoch 40/100\n",
      "8861/8861 [==============================] - 2s 215us/sample - loss: 0.3595 - R2: 0.6492 - val_loss: 0.3383 - val_R2: 0.6795\n",
      "Epoch 41/100\n",
      "8861/8861 [==============================] - 2s 199us/sample - loss: 0.3547 - R2: 0.6586 - val_loss: 0.3266 - val_R2: 0.6870\n",
      "Epoch 42/100\n",
      "8861/8861 [==============================] - 2s 199us/sample - loss: 0.3506 - R2: 0.6660 - val_loss: 0.3240 - val_R2: 0.6977\n",
      "Epoch 43/100\n",
      "8861/8861 [==============================] - 2s 200us/sample - loss: 0.3506 - R2: 0.6623 - val_loss: 0.3292 - val_R2: 0.6905\n",
      "Epoch 44/100\n",
      "8192/8861 [==========================>...] - ETA: 0s - loss: 0.3583 - R2: 0.6625\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.\n",
      "8861/8861 [==============================] - 2s 197us/sample - loss: 0.3527 - R2: 0.6603 - val_loss: 0.3341 - val_R2: 0.6861\n",
      "Epoch 45/100\n",
      "8861/8861 [==============================] - 2s 200us/sample - loss: 0.3542 - R2: 0.6556 - val_loss: 0.3266 - val_R2: 0.6955\n",
      "Epoch 46/100\n",
      "8861/8861 [==============================] - 2s 207us/sample - loss: 0.3547 - R2: 0.6585 - val_loss: 0.3235 - val_R2: 0.6973\n",
      "Epoch 47/100\n",
      "8861/8861 [==============================] - 2s 213us/sample - loss: 0.3490 - R2: 0.6664 - val_loss: 0.3214 - val_R2: 0.7012\n",
      "Epoch 48/100\n",
      "8861/8861 [==============================] - 2s 212us/sample - loss: 0.3460 - R2: 0.6680 - val_loss: 0.3217 - val_R2: 0.6979\n",
      "Epoch 49/100\n",
      "8861/8861 [==============================] - 2s 200us/sample - loss: 0.3461 - R2: 0.6674 - val_loss: 0.3198 - val_R2: 0.7026\n",
      "Epoch 50/100\n",
      "8861/8861 [==============================] - 2s 198us/sample - loss: 0.3461 - R2: 0.6675 - val_loss: 0.3193 - val_R2: 0.7001\n",
      "Epoch 51/100\n",
      "8861/8861 [==============================] - 2s 200us/sample - loss: 0.3479 - R2: 0.6645 - val_loss: 0.3219 - val_R2: 0.6993\n",
      "Epoch 52/100\n",
      "8861/8861 [==============================] - 2s 199us/sample - loss: 0.3468 - R2: 0.6694 - val_loss: 0.3194 - val_R2: 0.6953\n",
      "Epoch 53/100\n",
      "8861/8861 [==============================] - 2s 200us/sample - loss: 0.3458 - R2: 0.6676 - val_loss: 0.3244 - val_R2: 0.7005\n",
      "Epoch 54/100\n",
      "8861/8861 [==============================] - 2s 199us/sample - loss: 0.3466 - R2: 0.6707 - val_loss: 0.3227 - val_R2: 0.6964\n",
      "Epoch 55/100\n",
      "8861/8861 [==============================] - 2s 205us/sample - loss: 0.3451 - R2: 0.6664 - val_loss: 0.3200 - val_R2: 0.6941\n",
      "Epoch 56/100\n",
      "8861/8861 [==============================] - 2s 213us/sample - loss: 0.3449 - R2: 0.6700 - val_loss: 0.3187 - val_R2: 0.6942\n",
      "Epoch 57/100\n",
      "8861/8861 [==============================] - 2s 213us/sample - loss: 0.3439 - R2: 0.6725 - val_loss: 0.3215 - val_R2: 0.7043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "8861/8861 [==============================] - 2s 204us/sample - loss: 0.3463 - R2: 0.6694 - val_loss: 0.3222 - val_R2: 0.6963\n",
      "Epoch 59/100\n",
      "8861/8861 [==============================] - 2s 217us/sample - loss: 0.3455 - R2: 0.6646 - val_loss: 0.3186 - val_R2: 0.6974\n",
      "Epoch 60/100\n",
      "8861/8861 [==============================] - 2s 214us/sample - loss: 0.3438 - R2: 0.6688 - val_loss: 0.3214 - val_R2: 0.7022\n",
      "Epoch 61/100\n",
      "8861/8861 [==============================] - 2s 214us/sample - loss: 0.3470 - R2: 0.6664 - val_loss: 0.3228 - val_R2: 0.6971\n",
      "Epoch 62/100\n",
      "8861/8861 [==============================] - 2s 219us/sample - loss: 0.3484 - R2: 0.6658 - val_loss: 0.3303 - val_R2: 0.6934\n",
      "Epoch 63/100\n",
      "8861/8861 [==============================] - 2s 229us/sample - loss: 0.3510 - R2: 0.6635 - val_loss: 0.3204 - val_R2: 0.7033\n",
      "Epoch 64/100\n",
      "8861/8861 [==============================] - 2s 230us/sample - loss: 0.3465 - R2: 0.6647 - val_loss: 0.3208 - val_R2: 0.6972\n",
      "Epoch 65/100\n",
      "8861/8861 [==============================] - 2s 236us/sample - loss: 0.3443 - R2: 0.6683 - val_loss: 0.3185 - val_R2: 0.6985\n",
      "Epoch 66/100\n",
      "8861/8861 [==============================] - 2s 211us/sample - loss: 0.3458 - R2: 0.6675 - val_loss: 0.3211 - val_R2: 0.7022\n",
      "Epoch 67/100\n",
      "8861/8861 [==============================] - 2s 214us/sample - loss: 0.3453 - R2: 0.6699 - val_loss: 0.3193 - val_R2: 0.7019\n",
      "Epoch 68/100\n",
      "8861/8861 [==============================] - 2s 211us/sample - loss: 0.3437 - R2: 0.6673 - val_loss: 0.3183 - val_R2: 0.7021\n",
      "Epoch 69/100\n",
      "8861/8861 [==============================] - 2s 213us/sample - loss: 0.3425 - R2: 0.6677 - val_loss: 0.3173 - val_R2: 0.7079\n",
      "Epoch 70/100\n",
      "8861/8861 [==============================] - 2s 212us/sample - loss: 0.3424 - R2: 0.6725 - val_loss: 0.3185 - val_R2: 0.7015\n",
      "Epoch 71/100\n",
      "8861/8861 [==============================] - 2s 203us/sample - loss: 0.3470 - R2: 0.6669 - val_loss: 0.3283 - val_R2: 0.6823\n",
      "Epoch 72/100\n",
      "8192/8861 [==========================>...] - ETA: 0s - loss: 0.3537 - R2: 0.6678\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 0.0037499999161809683.\n",
      "8861/8861 [==============================] - 2s 232us/sample - loss: 0.3467 - R2: 0.6681 - val_loss: 0.3241 - val_R2: 0.6977\n",
      "Epoch 73/100\n",
      "8861/8861 [==============================] - 2s 213us/sample - loss: 0.3455 - R2: 0.6699 - val_loss: 0.3203 - val_R2: 0.6982\n",
      "Epoch 74/100\n",
      "8861/8861 [==============================] - 2s 203us/sample - loss: 0.3433 - R2: 0.6721 - val_loss: 0.3165 - val_R2: 0.7056\n",
      "Epoch 75/100\n",
      "8861/8861 [==============================] - 2s 197us/sample - loss: 0.3410 - R2: 0.6727 - val_loss: 0.3172 - val_R2: 0.6958\n",
      "Epoch 76/100\n",
      "8861/8861 [==============================] - 2s 202us/sample - loss: 0.3413 - R2: 0.6748 - val_loss: 0.3168 - val_R2: 0.7021\n",
      "Epoch 77/100\n",
      "8861/8861 [==============================] - 2s 199us/sample - loss: 0.3404 - R2: 0.6787 - val_loss: 0.3169 - val_R2: 0.6995\n",
      "Epoch 78/100\n",
      "8861/8861 [==============================] - 2s 199us/sample - loss: 0.3402 - R2: 0.6767 - val_loss: 0.3178 - val_R2: 0.7044\n",
      "Epoch 79/100\n",
      "8861/8861 [==============================] - 2s 204us/sample - loss: 0.3410 - R2: 0.6746 - val_loss: 0.3192 - val_R2: 0.6995\n",
      "Epoch 80/100\n",
      "8861/8861 [==============================] - 2s 209us/sample - loss: 0.3431 - R2: 0.6708 - val_loss: 0.3164 - val_R2: 0.6968\n",
      "Epoch 81/100\n",
      "8861/8861 [==============================] - 2s 244us/sample - loss: 0.3409 - R2: 0.6766 - val_loss: 0.3198 - val_R2: 0.6914\n",
      "Epoch 82/100\n",
      "8861/8861 [==============================] - 2s 240us/sample - loss: 0.3418 - R2: 0.6732 - val_loss: 0.3162 - val_R2: 0.7045\n",
      "Epoch 83/100\n",
      "8861/8861 [==============================] - 2s 216us/sample - loss: 0.3401 - R2: 0.6769 - val_loss: 0.3167 - val_R2: 0.7030\n",
      "Epoch 84/100\n",
      "8861/8861 [==============================] - 2s 203us/sample - loss: 0.3397 - R2: 0.6759 - val_loss: 0.3162 - val_R2: 0.7084\n",
      "Epoch 85/100\n",
      "8861/8861 [==============================] - 2s 200us/sample - loss: 0.3394 - R2: 0.6744 - val_loss: 0.3158 - val_R2: 0.6985\n",
      "Epoch 86/100\n",
      "8861/8861 [==============================] - 2s 205us/sample - loss: 0.3395 - R2: 0.6757 - val_loss: 0.3163 - val_R2: 0.6963\n",
      "Epoch 87/100\n",
      "8192/8861 [==========================>...] - ETA: 0s - loss: 0.3391 - R2: 0.6767Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 0.0018749999580904841.\n",
      "8861/8861 [==============================] - 2s 215us/sample - loss: 0.3402 - R2: 0.6742 - val_loss: 0.3172 - val_R2: 0.7033\n",
      "Epoch 00087: early stopping\n"
     ]
    }
   ],
   "source": [
    "es     = callbacks.EarlyStopping(monitor='val_R2', mode='max', verbose=1, patience=30, \n",
    "                                 min_delta=0.01, restore_best_weights=True)\n",
    "reduce = callbacks.ReduceLROnPlateau(monitor='val_R2', factor=0.5, patience=15, verbose=1, \n",
    "                                     mode='max', min_delta=0.01, cooldown=0, min_lr=LEARNING_RATE / 100)\n",
    "tnan   = callbacks.TerminateOnNaN()\n",
    "\n",
    "model.compile(loss='mse', metrics=[R2], optimizer=optimizers.Adam(lr=LEARNING_RATE))\n",
    "model.fit(x_train, y_train, epochs=EPOCH_NUMBER, batch_size=1024, validation_split=0.3,   # Train the model\n",
    "          callbacks=[es, reduce, tnan])\n",
    "model.save(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = models.load_model(model_path, custom_objects={'R2': xutils.R2})\n",
    "pred_train = eval_model.predict(x_train, batch_size=1024)  # Make predictions\n",
    "pred_test  = eval_model.predict(x_test, batch_size=1024)  # Make predictions\n",
    "\n",
    "print(f\"NSE for the training data: {xutils.cal_nse(y_train, pred_train):.3f}\")\n",
    "print(f\"NSE for the testing data:  {xutils.cal_nse(y_test, pred_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrodata.loc[train_dates, ['flow_pred']] = pred_train * scale_params['train_y_std'] + scale_params['train_y_mean']\n",
    "hydrodata.to_csv(r'C:\\TsaiHejiang\\CMZ\\results\\results_train_daily.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the train and test\n",
    "hydrodata.loc[test_dates,  ['flow_pred']] = pred_test  * scale_params['train_y_std'] + scale_params['train_y_mean']\n",
    "hydrodata.to_csv(r'C:\\TsaiHejiang\\CMZ\\results\\results_daily.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
